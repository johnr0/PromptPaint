{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnr0/InGen/blob/main/InGen_Server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgzAS-vWP5q2"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St9ao0bH8B8c"
      },
      "outputs": [],
      "source": [
        "!pip install flask==2.0.1\n",
        "!pip install flask_api==2.0\n",
        "# !pip install python-socketio==4.6.0\n",
        "# !pip install python-engineio==3.13.2\n",
        "!pip install flask_socketio==5.1.0\n",
        "!pip install flask_cors==3.0.3\n",
        "# !pip install Werkzeug==0.16.1\n",
        "\n",
        "# !pip install diffusers==0.9\n",
        "!pip install diffusers==0.4\n",
        "!pip install transformers scipy ftfy\n",
        "!pip install \"ipywidgets>=7,<8\"\n",
        "!pip install gevent-websocket\n",
        "\n",
        "\n",
        "!pip install pyngrok\n",
        "!ngrok authtoken 1pDdjhcWhyh8VUFRTYCnGgCOI8g_3f56gBgxvREzT4gfAWXp2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiOa3AanP0V3"
      },
      "source": [
        "# Logging in with Huggingface Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUU23aKxhh-v"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmhlx0GP-AW"
      },
      "source": [
        "# Installing Ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0a2bMeTGY8e"
      },
      "outputs": [],
      "source": [
        "# ngrok install\n",
        "!wget --quiet https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSrvWFBZhxE4"
      },
      "source": [
        "# Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRGA9kn5hy0j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
        "# vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=True)\n",
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", revision='fp16', torch_dtype=torch.float16, subfolder=\"vae\", use_auth_token=True)\n",
        "# vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-inpainting\", revision='fp16', torch_dtype=torch.float16, subfolder=\"vae\", use_auth_token=True)\n",
        "# vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", revision='fp16', torch_dtype=torch.float16, subfolder=\"vae\", use_auth_token=True)\n",
        "\n",
        "# vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-inpainting\", revision='fp16', torch_dtype=torch.float16, subfolder=\"vae\", use_auth_token=True)\n",
        "\n",
        "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = text_encoder.half()\n",
        "\n",
        "# 3. The UNet model for generating the latents.\n",
        "# unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=True)\n",
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", revision='fp16', torch_dtype=torch.float16, subfolder=\"unet\", use_auth_token=True)\n",
        "# unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-inpainting\", revision='fp16', torch_dtype=torch.float16, subfolder=\"unet\", use_auth_token=True)\n",
        "# unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2-inpainting\", revision='fp16', torch_dtype=torch.float16, subfolder=\"unet\", use_auth_token=True)\n",
        "\n",
        "from diffusers import DDIMScheduler\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "from torch import autocast\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device) \n",
        "\n",
        "embedded_text_prompts = {}\n",
        "\n",
        "\n",
        "\n",
        "def text_prompt_embed(t):\n",
        "  with torch.no_grad():\n",
        "    if t in embedded_text_prompts:\n",
        "      return embedded_text_prompts[t]\n",
        "    else: \n",
        "      text_input = tokenizer([t], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "      text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "      embedded_text_prompts[t] = text_embeddings\n",
        "      return text_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2AGO2c7lGEc"
      },
      "outputs": [],
      "source": [
        "# variables that can change dynamically during generation\n",
        "directional_prompt_inputs = {}\n",
        "\n",
        "directional_prompt_inputs_proto = {}\n",
        "\n",
        "prompt_inputs = {}\n",
        "\n",
        "prompt_inputs_proto = {}\n",
        "\n",
        "prompts_whole_proto = {}\n",
        "\n",
        "guidance_scale = {}\n",
        "\n",
        "threads = {}\n",
        "\n",
        "sub_threads = {}\n",
        "\n",
        "gen_stop = {}\n",
        "\n",
        "latents_list = {}\n",
        "dir_prompt_list = {}\n",
        "prompt_list = {}\n",
        "guidance_scale_list = {}\n",
        "prompts_whole_list = {}\n",
        "\n",
        "lms = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pNHKMSXQBE5"
      },
      "source": [
        "# Initializing Ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfcl69GCGf-s"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "ngrok_tunnel = ngrok.connect(5001)\n",
        "ngrok_tunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr7YmuadQK9w"
      },
      "source": [
        "# Run server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz-dTsu58gxY"
      },
      "outputs": [],
      "source": [
        "\n",
        "from flask_socketio import SocketIO, emit\n",
        "from flask import Flask, request, copy_current_request_context\n",
        "from flask_cors import CORS\n",
        "from random import gauss, random\n",
        "# from threading import Thread, Event\n",
        "from time import sleep\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from threading import Thread\n",
        "from gevent.pywsgi import WSGIServer\n",
        "from geventwebsocket.handler import WebSocketHandler\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['SECRET_KEY'] = 'secret!'\n",
        "\n",
        "socketio = SocketIO(app, cors_allowed_origins='*', async_mode='gevent', ping_interval=1, max_http_buffer_size=10e8)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "\n",
        "@app.route('/hello')\n",
        "def hello():\n",
        "    return \"Hello World!\"\n",
        "\n",
        "\n",
        "# Handle the webapp connecting to the websocket\n",
        "@socketio.on('connect')\n",
        "def test_connect():\n",
        "    print('someone connected to websocket:', request.sid)\n",
        "    emit('connect', {'data': 'Connected! ayy'})\n",
        "    \n",
        "@socketio.on('disconnect')\n",
        "def test_connect():\n",
        "    print(request.sid)\n",
        "    print('someone disconnected to websocket')\n",
        "    emit('disconnect', {'data': 'Disconnected! ayy'})\n",
        "\n",
        "# Handle the webapp connecting to the websocket, including namespace for testing\n",
        "@socketio.on('connect', namespace='/devices')\n",
        "def test_connect2():\n",
        "    print('someone connected to websocket!')\n",
        "    emit('responseMessage', {'data': 'Connected devices! ayy'})\n",
        "\n",
        "\n",
        "def preprocess_mask(mask):\n",
        "    mask = mask.convert(\"L\")\n",
        "    w, h = mask.size\n",
        "    if w < h:\n",
        "      h = int(h*(512/w))\n",
        "      w = 512\n",
        "    else:\n",
        "      w = int(w*(512/h))\n",
        "      h = 512\n",
        "      \n",
        "\n",
        "    # if w > 512:\n",
        "    #   h = int(h * (512/w))\n",
        "    #   w = 512\n",
        "    # if h > 512:\n",
        "    #   w = int(w*(512/h))\n",
        "    #   h = 512\n",
        "    \n",
        "    w, h = map(lambda x: x - x % 64, (w, h)) \n",
        "    w //= 8\n",
        "    h //= 8\n",
        "\n",
        "    mask = mask.resize((w, h), resample=Image.LANCZOS)\n",
        "\n",
        "    # mask = np.array(mask).astype(np.float32) / 255.0\n",
        "    mask = np.array(mask).astype(np.float16) / 255.0\n",
        "    mask = np.tile(mask, (4,1,1))\n",
        "    mask = mask[None].transpose(0,1,2,3)\n",
        "    mask[np.where(mask !=0.0)]=1.0\n",
        "    mask = torch.from_numpy(mask)\n",
        "    return mask\n",
        "\n",
        "def preprocess(image):\n",
        "    image = image.convert('RGB')\n",
        "    w, h = image.size\n",
        "    print(w,h)\n",
        "    if w < h:\n",
        "      h = int(h*(512/w))\n",
        "      w = 512\n",
        "    else:\n",
        "      w = int(w*(512/h))\n",
        "      h = 512\n",
        "    # if w > 512:\n",
        "    #   h = int(h * (512/w))\n",
        "    #   w = 512\n",
        "    # if h > 512:\n",
        "    #   w = int(w*(512/h))\n",
        "    #   h = 512\n",
        "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64, 32 can sometimes result in tensor mismatch errors\n",
        "\n",
        "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
        "    print(image.size)\n",
        "    # image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.0 * image - 1.0\n",
        "\n",
        "def numpy_to_pil(images):\n",
        "    \"\"\"\n",
        "    Convert a numpy image or a batch of images to a PIL image.\n",
        "    \"\"\"\n",
        "    if images.ndim == 3:\n",
        "            images = images[None, ...]\n",
        "    images = (images * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image).convert('RGBA') for image in images]\n",
        "    return pil_images\n",
        "\n",
        "@socketio.on('test')\n",
        "def handle_message_t(message):\n",
        "    print(message)\n",
        "\n",
        "@socketio.on('gen_stop')\n",
        "def gen_stop_message_t(message):\n",
        "    gen_stop[message['stroke_id']] = True\n",
        "\n",
        "@socketio.on('guidance_scale_update')\n",
        "def guidance_scale_message_t(message):\n",
        "  guidance_scale[message['stroke_id']] = message['guidance_scale']\n",
        "\n",
        "@socketio.on('prompts_update')\n",
        "def prompts_message_t(message):\n",
        "  text_prompts = message['text_prompts']\n",
        "  text_prompt_weights = message['text_prompt_weights']\n",
        "\n",
        "  text_prompt_embedding = None\n",
        "  tpw = 0\n",
        "  for tp_idx, text_prompt in enumerate(text_prompts):\n",
        "    # print('inloop1', time.time()-now)\n",
        "    cur_embedding = text_prompt_embed(text_prompt)\n",
        "    # print('inloop2', time.time()-now)\n",
        "    if text_prompt_embedding==None:\n",
        "      text_prompt_embedding = cur_embedding*text_prompt_weights[tp_idx]\n",
        "    else:\n",
        "      text_prompt_embedding = text_prompt_embedding + cur_embedding*text_prompt_weights[tp_idx]\n",
        "    tpw = tpw + text_prompt_weights[tp_idx]\n",
        "    # print('inloop3', time.time()-now)\n",
        "  text_prompt_embedding = text_prompt_embedding/tpw\n",
        "\n",
        "  # uncond_embeddings = text_prompt_embed('')\n",
        "  # text_embeddings = torch.cat([uncond_embeddings, text_prompt_embedding])\n",
        "\n",
        "  prompts_whole_proto[message['stroke_id']] = message['prompts_proto']\n",
        "  prompt_inputs[message['stroke_id']] = text_prompt_embedding #text_embeddings\n",
        "  prompt_inputs_proto[message['stroke_id']] = message['selected_prompts_proto']\n",
        "\n",
        "@socketio.on('directional_prompts_update')\n",
        "def directional_prompts_message_t(message):\n",
        "  directional_prompts = message['directional_prompts']\n",
        "\n",
        "  directional_vector = None\n",
        "  for directional_prompt in directional_prompts:\n",
        "    if directional_prompt['value']==0:\n",
        "      continue\n",
        "    if directional_vector == None:\n",
        "      directional_vector = float(directional_prompt['value'])/100.0 * (text_prompt_embed(directional_prompt['promptB'])-text_prompt_embed(directional_prompt['promptA']))\n",
        "    else:\n",
        "      directional_vector = directional_vector + float(directional_prompt['value'])/100.0 * (text_prompt_embed(directional_prompt['promptB'])-text_prompt_embed(directional_prompt['promptA']))\n",
        "  directional_prompt_inputs[message['stroke_id']] = directional_vector\n",
        "  directional_prompt_inputs_proto[message['stroke_id']] = message['directional_prompts_proto']        \n",
        "\n",
        "\n",
        "@socketio.on('gen_start')\n",
        "@torch.no_grad()\n",
        "def handle_message(message):\n",
        "    @copy_current_request_context\n",
        "    def handle_message_thread(message):\n",
        "      try:\n",
        "        with torch.no_grad():\n",
        "          now = time.time()\n",
        "\n",
        "          gen_stop[message['stroke_id']] = False\n",
        "\n",
        "          guidance_scale[message['stroke_id']] = message['guidance_scale']\n",
        "          text_prompts = message['text_prompts']\n",
        "          text_prompt_weights = message['text_prompt_weights']\n",
        "          directional_prompts = message['directional_prompts']\n",
        "\n",
        "          if message['gen_tick']==0:\n",
        "            latents_list[message['stroke_id']] = []\n",
        "            dir_prompt_list[message['stroke_id']] = []\n",
        "            prompt_list[message['stroke_id']] = []\n",
        "            guidance_scale_list[message['stroke_id']] = []\n",
        "            prompts_whole_list[message['stroke_id']] = []\n",
        "          else:\n",
        "            # pop things until the tick\n",
        "            latents_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "            dir_prompt_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "            prompt_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "            guidance_scale_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "            prompts_whole_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "\n",
        "          layer_img_o = Image.open(BytesIO(base64.b64decode(message['layer_img'].split(\",\",1)[1])))\n",
        "          area_img = Image.new(\"RGBA\", layer_img_o.size, \"WHITE\")\n",
        "          area_img_o = Image.open(BytesIO(base64.b64decode(message['area_img'].split(\",\",1)[1])))\n",
        "          area_img.paste(area_img_o, (0,0), area_img_o)\n",
        "          \n",
        "          \n",
        "\n",
        "\n",
        "          overcoat_ratio = message['overcoat_ratio']\n",
        "          seed = message['seed']\n",
        "          generator = torch.Generator(device='cuda', )\n",
        "          generator.manual_seed(seed) \n",
        "\n",
        "          # set mask from area_img\n",
        "          area_mask = preprocess_mask(area_img)\n",
        "          area_mask = area_mask.to(torch_device)\n",
        "\n",
        "          \n",
        "\n",
        "          # add black or white background to the layer image\n",
        "          layer_img_back = Image.new(\"RGBA\", layer_img_o.size, \"WHITE\")\n",
        "          layer_img_back.paste(layer_img_o, (0, 0), layer_img_o)\n",
        "          layer_img_back.convert('RGB')\n",
        "          layer_img = preprocess(layer_img_back)\n",
        "          init_latents = vae.encode(layer_img.to(torch_device)).latent_dist.sample()\n",
        "          init_latents = 0.18215 * init_latents\n",
        "\n",
        "          init_latents = init_latents.half()\n",
        "\n",
        "          noise = torch.randn(init_latents.shape, generator=generator, device=torch_device, dtype=torch.float16)\n",
        "\n",
        "          num_inference_steps = message['steps']\n",
        "          scheduler.set_timesteps(num_inference_steps)\n",
        "          t_init = scheduler.timesteps[message['gen_tick']]\n",
        "          # if message['gen_tick'] < int((1-overcoat_ratio)*num_inference_steps):\n",
        "          layer_array = np.copy(np.asarray(layer_img_o))\n",
        "\n",
        "          alphas = np.ones(layer_array[:,:,3,None].shape)*255\n",
        "          layer_array = np.concatenate((layer_array[:,:,3,None], layer_array[:,:,3,None], layer_array[:,:,3,None], alphas), axis = 2)\n",
        "          layer_array = np.array(layer_array, dtype = np.uint8)\n",
        "      \n",
        "          layer_img_mask = Image.fromarray(layer_array)\n",
        "          layer_mask = preprocess_mask(layer_img_mask)\n",
        "          layer_mask = layer_mask.to(torch_device)\n",
        "\n",
        "          last_mask = None\n",
        "          if 'last_img' in message:\n",
        "            last_img = Image.new(\"RGBA\", layer_img_o.size, \"WHITE\")\n",
        "            last_img_o = Image.open(BytesIO(base64.b64decode(message['last_img'].split(\",\",1)[1])))\n",
        "            last_img.paste(last_img_o, (0,0), last_img_o)\n",
        "            last_mask = preprocess_mask(last_img)\n",
        "            last_mask = last_mask.to(torch_device)\n",
        "            last_mask = 1+last_mask-area_mask\n",
        "            # last_mask = layer_mask\n",
        "\n",
        "            last_latents = torch.Tensor(message['last_latents'])\n",
        "            last_latents = last_latents.to(torch_device)\n",
        "            # lms.append(last_latents)\n",
        "            # lms.append(last_img)\n",
        "\n",
        "\n",
        "          print(time.time()-now)\n",
        "          if message['gen_tick']==0:   \n",
        "            \n",
        "            # latents = noise # start from the purse noise\n",
        "\n",
        "            latents = (1-area_mask)* (1-layer_mask) * noise + (1-(1-area_mask)* (1-layer_mask)) * init_latents # In / Out\n",
        "            latents = (1-overcoat_ratio) * latents + overcoat_ratio * noise\n",
        "            \n",
        "            # scheduler.add_noise(latents, noise, t-1)\n",
        "          else:\n",
        "            # print('stored latent is used')\n",
        "            latents = torch.Tensor(message['latents'])\n",
        "            latents = latents.type(torch.float16)\n",
        "            latents = latents.to(torch_device)\n",
        "\n",
        "\n",
        "          # print(latents.size())\n",
        "\n",
        "\n",
        "          print(time.time()-now)\n",
        "\n",
        "          # set directional prompt embeddings\n",
        "          directional_vector = None\n",
        "          for directional_prompt in directional_prompts:\n",
        "            if directional_prompt['value']==0:\n",
        "              continue\n",
        "            if directional_vector == None:\n",
        "              directional_vector = float(directional_prompt['value'])/100.0 * (text_prompt_embed(directional_prompt['promptB'])-text_prompt_embed(directional_prompt['promptA']))\n",
        "            else:\n",
        "              directional_vector = directional_vector + float(directional_prompt['value'])/100.0 * (text_prompt_embed(directional_prompt['promptB'])-text_prompt_embed(directional_prompt['promptA']))\n",
        "          \n",
        "          text_prompt_embedding = None\n",
        "          tpw = 0\n",
        "          for tp_idx, text_prompt in enumerate(text_prompts):\n",
        "            # print('inloop1', time.time()-now)\n",
        "            cur_embedding = text_prompt_embed(text_prompt)\n",
        "            # print('inloop2', time.time()-now)\n",
        "            if text_prompt_embedding==None:\n",
        "              text_prompt_embedding = cur_embedding*text_prompt_weights[tp_idx]\n",
        "            else:\n",
        "              text_prompt_embedding = text_prompt_embedding + cur_embedding*text_prompt_weights[tp_idx]\n",
        "            tpw = tpw + text_prompt_weights[tp_idx]\n",
        "            # print('inloop3', time.time()-now)\n",
        "          text_prompt_embedding = text_prompt_embedding/tpw\n",
        "\n",
        "          uncond_embeddings = text_prompt_embed('')\n",
        "          # if directional_vector!=None:\n",
        "          #   text_embeddings = torch.cat([uncond_embeddings, text_prompt_embedding+directional_vector])\n",
        "          # else:\n",
        "          #   text_embeddings = torch.cat([uncond_embeddings, text_prompt_embedding])\n",
        "\n",
        "          directional_prompt_inputs[message['stroke_id']] = directional_vector\n",
        "          directional_prompt_inputs_proto[message['stroke_id']] = message['directional_prompts_proto']\n",
        "          prompts_whole_proto[message['stroke_id']] = message['prompts_proto']\n",
        "          prompt_inputs[message['stroke_id']] = text_prompt_embedding\n",
        "          prompt_inputs_proto[message['stroke_id']] = message['selected_prompts_proto']\n",
        "          # print(uncond_embeddings.size(), text_prompt_embedding.size())\n",
        "\n",
        "          print(time.time()-now)\n",
        "\n",
        "          # num inference steps should be fixed\n",
        "          gen_duration = np.min([message['gen_tick']+message['gen_duration'], message['steps']])\n",
        "          gen_duration = np.max([1, gen_duration])\n",
        "          \n",
        "          print(scheduler.timesteps, scheduler.timesteps[message['gen_tick']:])\n",
        "          # sub_threads[message['stroke_id']] = Thread(target=sendoutIntermediate, args = (message['gen_tick'], layer_img_o, area_img_o, message['area_img'], message['stroke_id'], gen_duration))\n",
        "          # sub_threads[message['stroke_id']].start()\n",
        "\n",
        "          print(time.time()-now)\n",
        "\n",
        "          \n",
        "          for i, t in enumerate(scheduler.timesteps[message['gen_tick']:gen_duration]):\n",
        "            print(i, 'start', guidance_scale[message['stroke_id']])\n",
        "            if gen_stop[message['stroke_id']]:\n",
        "              return\n",
        "            # Do something about generation\n",
        "            latent_model_input = torch.cat([latents] * 2)\n",
        "            # predict the noise residual\n",
        "            with torch.no_grad():\n",
        "              if directional_prompt_inputs[message['stroke_id']] == None:\n",
        "                text_embeddings = torch.cat([uncond_embeddings, prompt_inputs[message['stroke_id']]])\n",
        "              else:\n",
        "                text_embeddings = torch.cat([uncond_embeddings, prompt_inputs[message['stroke_id']]+directional_prompt_inputs[message['stroke_id']]])\n",
        "              print('unet pipeline0', time.time()-now, latent_model_input.is_cuda, type(t), t.shape, text_embeddings.is_cuda)\n",
        "              noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "              print(noise_pred.type())\n",
        "            # perform guidance\n",
        "            print('unet pipeline1', time.time()-now)\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            \n",
        "            noise_pred = noise_pred_uncond + guidance_scale[message['stroke_id']] * (noise_pred_text - noise_pred_uncond)\n",
        "            latents = scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
        "            print('unet pipeline2', time.time()-now)\n",
        "\n",
        "            # t_noise = torch.randn(latents.shape, device=torch_device)\n",
        "            if t > 1:\n",
        "              # when over overcoat ratio\n",
        "              if last_mask == None:\n",
        "                if i >= int((1-overcoat_ratio)*num_inference_steps):\n",
        "                  # init_latents_proper = scheduler.add_noise(init_latents, noise, t-1)\n",
        "                  proto_prod = torch.ones(1,1,1,1, device=torch_device, dtype=torch.float16)\n",
        "                  sqrt_alpha_prod = scheduler.alphas_cumprod[t-1] ** 0.5\n",
        "                  sqrt_one_minus_alpha_prod = (1 - scheduler.alphas_cumprod[t-1]) ** 0.5\n",
        "                  sqrt_alpha_prod=sqrt_alpha_prod * proto_prod\n",
        "                  sqrt_one_minus_alpha_prod=sqrt_one_minus_alpha_prod * proto_prod\n",
        "                  init_latents_proper = sqrt_alpha_prod * init_latents + sqrt_one_minus_alpha_prod * noise\n",
        "                  latents = init_latents_proper * area_mask    +    latents * (1-area_mask)\n",
        "                  print('post process 1')\n",
        "                else:\n",
        "                  proto_prod = torch.ones(1,1,1,1, device=torch_device, dtype=torch.float16)\n",
        "                  sqrt_alpha_prod = scheduler.alphas_cumprod[t-1] ** 0.5\n",
        "                  sqrt_one_minus_alpha_prod = (1 - scheduler.alphas_cumprod[t-1]) ** 0.5\n",
        "                  sqrt_alpha_prod=sqrt_alpha_prod * proto_prod\n",
        "                  sqrt_one_minus_alpha_prod=sqrt_one_minus_alpha_prod * proto_prod\n",
        "                  init_latents_proper = sqrt_alpha_prod * init_latents + sqrt_one_minus_alpha_prod * noise\n",
        "                  # init_latents_proper = scheduler.add_noise(init_latents, noise, t-1)\n",
        "                  latents = (1-area_mask)* (1-layer_mask) * latents + (1-(1-area_mask)* (1-layer_mask)) * init_latents_proper # In / Out\n",
        "                  print('post process 2')\n",
        "              else:\n",
        "                print('this is last')\n",
        "                # if i >= int((1-overcoat_ratio)*num_inference_steps):\n",
        "                  # init_latents_proper = scheduler.add_noise(init_latents, noise, t-1)\n",
        "                proto_prod = torch.ones(1,1,1,1, device=torch_device, dtype=torch.float16)\n",
        "                sqrt_alpha_prod = scheduler.alphas_cumprod[t-1] ** 0.5\n",
        "                sqrt_one_minus_alpha_prod = (1 - scheduler.alphas_cumprod[t-1]) ** 0.5\n",
        "                sqrt_alpha_prod=sqrt_alpha_prod * proto_prod\n",
        "                sqrt_one_minus_alpha_prod=sqrt_one_minus_alpha_prod * proto_prod\n",
        "                init_latents_proper = sqrt_alpha_prod * init_latents + sqrt_one_minus_alpha_prod * noise\n",
        "                last_latents_proper = scheduler.add_noise(last_latents, noise, t-1)\n",
        "                # latents = init_latents_proper * (last_mask+area_mask-1)    +    latents * (1-area_mask) + last_latents_proper * (1-last_mask)\n",
        "                latents = init_latents_proper * (area_mask-layer_mask*area_mask)    +    latents * (1-area_mask) + last_latents_proper * (layer_mask* area_mask)\n",
        "                # lms.append(init_latents_proper * (last_mask+area_mask-1))\n",
        "                # lms.append(latents * (1-area_mask) )\n",
        "                # lms.append(last_latents_proper * (1-last_mask))\n",
        "                print('post process 1')\n",
        "                # else:\n",
        "                #   proto_prod = torch.ones(1,1,1,1, device=torch_device)\n",
        "                #   sqrt_alpha_prod = scheduler.alphas_cumprod[t-1] ** 0.5\n",
        "                #   sqrt_one_minus_alpha_prod = (1 - scheduler.alphas_cumprod[t-1]) ** 0.5\n",
        "                #   sqrt_alpha_prod=sqrt_alpha_prod * proto_prod\n",
        "                #   sqrt_one_minus_alpha_prod=sqrt_one_minus_alpha_prod * proto_prod\n",
        "                #   init_latents_proper = sqrt_alpha_prod * init_latents + sqrt_one_minus_alpha_prod * noise\n",
        "                #   last_latents_proper = scheduler.add_noise(last_latents, noise, t-1)\n",
        "                #   # init_latents_proper = scheduler.add_noise(init_latents, noise, t-1)\n",
        "                #   # latents = (1-area_mask)* (1-layer_mask) * latents + (last_mask-(1-area_mask)* (1-layer_mask)) * init_latents_proper + last_latents_proper * (1-last_mask) # In / Out\n",
        "                #   latents = init_latents_proper * (layer_mask-(1-last_mask))    +    latents * (1-last_mask) *(1-area_mask) + last_latents_proper * (1-last_mask)\n",
        "                #   print('post process 2')\n",
        "                \n",
        "              # when below overcoat ratio\n",
        "            else:\n",
        "              if last_mask==None:\n",
        "                latents = init_latents * area_mask    +    latents * (1-area_mask)\n",
        "              else:\n",
        "                # latents = init_latents* (last_mask+area_mask-1)    +    latents * (1-area_mask) + last_latents * (1-last_mask)\n",
        "                latents = init_latents * (area_mask-layer_mask*area_mask)    +    latents * (1-area_mask) + last_latents * (layer_mask*area_mask)\n",
        "              print('post process 3')\n",
        "            print('post process', time.time()-now)\n",
        "\n",
        "            latents_list[message['stroke_id']].append(latents)\n",
        "            dir_prompt_list[message['stroke_id']].append(directional_prompt_inputs_proto[message['stroke_id']])\n",
        "            prompt_list[message['stroke_id']].append(prompt_inputs_proto[message['stroke_id']])\n",
        "            guidance_scale_list[message['stroke_id']].append(guidance_scale[message['stroke_id']])\n",
        "            prompts_whole_list[message['stroke_id']].append(prompts_whole_proto[message['stroke_id']])\n",
        "\n",
        "            int_thread = Thread(target=sendoutIntermediate, args = (message['gen_tick']+i, layer_img_o, area_img_o, message['area_img'], message['stroke_id'], gen_duration))\n",
        "            int_thread.start()\n",
        "      except:\n",
        "        emit('gen_failed', {'data':'gen failed!'})\n",
        "\n",
        "\n",
        "\n",
        "    @copy_current_request_context\n",
        "    def sendoutIntermediate(gen_tick, layer_img_o, area_img_o, area_img_d, stroke_id, gen_duration):\n",
        "      idx = gen_tick\n",
        "      ta = time.time()\n",
        "      with torch.no_grad():\n",
        "          \n",
        "          \n",
        "        if gen_stop[stroke_id]:\n",
        "          return\n",
        "        print('before emit', time.time()-ta)\n",
        "        latents = latents_list[stroke_id][idx]\n",
        "        dir_prompts_ = dir_prompt_list[stroke_id][idx]\n",
        "        prompts_ = prompt_list[stroke_id][idx]\n",
        "        prompts_whole_ = prompts_whole_list[stroke_id][idx]\n",
        "        guidance_scale_ = guidance_scale_list[stroke_id][idx]\n",
        "        print(latents.shape)\n",
        "        \n",
        "        output_img = 1 / 0.18215 * latents\n",
        "        output_img = vae.decode(output_img).sample\n",
        "        output_img = (output_img / 2 + 0.5).clamp(0, 1)\n",
        "        output_img = output_img.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        output_img = numpy_to_pil(output_img)[0]\n",
        "        output_img = output_img.resize((layer_img_o.size[0], layer_img_o.size[1]), resample=Image.LANCZOS)\n",
        "        output_array = np.asarray(output_img)\n",
        "        output_array = np.copy(output_array)\n",
        "        latents_rt = latents.tolist()\n",
        "        # print('within3', time.time()-now)\n",
        "\n",
        "        area_array = np.asarray(area_img_o)\n",
        "        area_array = np.where(area_array==255, 255, 0)\n",
        "        # print(area_array.shape, gaussian.shape)\n",
        "        output_array[:,:,3] = area_array[:,:,3]\n",
        "        # print('within4', time.time()-now)\n",
        "        output_img = Image.fromarray(output_array)\n",
        "        # print('within5', time.time()-now)\n",
        "        \n",
        "        # print('here?')\n",
        "        buffered = BytesIO()\n",
        "        output_img.save(buffered, format=\"PNG\")\n",
        "        output_img_send = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "        print('lets emit', time.time()-ta)\n",
        "        # emit('test', {'data':'test'+str(idx)})\n",
        "        idx = idx+1\n",
        "        emit('intermediate_gen', {'data':area_img_d.split(\",\",1)[0]+','+output_img_send, 'gen_tick':idx, 'stroke_id': stroke_id, 'latents': latents_rt, 'selected_prompts': prompts_, 'prompts':prompts_whole_, 'directional_prompts': dir_prompts_, 'guidance_scale': guidance_scale_})\n",
        "        print('thread end!')\n",
        "\n",
        "    threads[message['stroke_id']] = Thread(target=handle_message_thread , args = [message])\n",
        "    threads[message['stroke_id']].start()\n",
        "    emit('test', {'data':'gen started!'})\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "# socketio.run(app, host='0.0.0.0', debug=False, port=5001)\n",
        "http_server = WSGIServer(('',5001), app, handler_class=WebSocketHandler)\n",
        "http_server.serve_forever()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}